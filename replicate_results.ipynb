{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "replicate_results.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0TWcI2qt6hM",
        "colab_type": "code",
        "outputId": "e2f2cf41-1afe-47b7-e340-14e5266dfc61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/podgorskiy/ALAE.git\n",
        "%cd ALAE\n",
        "%set_env PYTHONPATH=/project/pylib/src:/env/python\n",
        "!pip install -r requirements.txt\n",
        "#Dowload Mdels\n",
        "!python training_artifacts/download_all.py\n",
        "#Upload Pictures:\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ALAE'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 2318 (delta 2), reused 2 (delta 0), pack-reused 2311\u001b[K\n",
            "Receiving objects: 100% (2318/2318), 208.04 MiB | 32.40 MiB/s, done.\n",
            "Resolving deltas: 100% (898/898), done.\n",
            "/content/ALAE\n",
            "env: PYTHONPATH=/project/pylib/src:/env/python\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (20.4)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (4.41.1)\n",
            "Collecting dlutils\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/bd/bb55d8fc3eac72f4eb5e3b54a0f4b1001ee0643481ad3fab030e657a34a1/dlutils-0.0.12-py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.0MB/s \n",
            "\u001b[?25hCollecting bimpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/8d/69868b08f61fb111f6486d77d5b9a912fb821208a64463a6bf2b431b72a7/bimpy-0.0.13-cp36-cp36m-manylinux2010_x86_64.whl (5.9MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9MB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.5.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (0.6.0+cu101)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.0)\n",
            "Collecting yacs\n",
            "  Downloading https://files.pythonhosted.org/packages/81/3b/40e876afde9f5ffa1cfdce10565aba85b0dc2e067ed551dfb566cfee6d4d/yacs-0.1.7-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (3.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->-r requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio->-r requirements.txt (line 2)) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->-r requirements.txt (line 10)) (0.22.2.post1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from yacs->-r requirements.txt (line 11)) (3.13)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 12)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 12)) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 12)) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 10)) (0.15.1)\n",
            "Installing collected packages: dlutils, bimpy, yacs\n",
            "Successfully installed bimpy-0.0.13 dlutils-0.0.12 yacs-0.1.7\n",
            "Downloading: model_submitted.pth\n",
            " 651584912\n",
            "Done\n",
            "Downloading: model_157.pth\n",
            " 651591168\n",
            "Done\n",
            "Downloading: model_194.pth\n",
            " 651503057\n",
            "Done\n",
            "Downloading: model_final.pth\n",
            " 107279879\n",
            "Done\n",
            "Downloading: model_final.pth\n",
            " 511800300\n",
            "Done\n",
            "Downloading: model_262r.pth\n",
            " 432005746\n",
            "Done\n",
            "Downloading: model_580r.pth\n",
            " 431932331\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiJ8mUBLt0f2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import torch.utils.data\n",
        "from net import *\n",
        "from model import Model\n",
        "from launcher import run\n",
        "from checkpointer import Checkpointer\n",
        "from dlutils.pytorch import count_parameters\n",
        "from defaults import get_cfg_defaults\n",
        "import lreq\n",
        "import logging\n",
        "from PIL import Image\n",
        "import bimpy\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "lreq.use_implicit_lreq.set(True)\n",
        "indices = [0, 1, 2, 3, 4, 10, 11, 17, 19]\n",
        "labels = [\"gender\",\n",
        "          \"smile\",\n",
        "          \"attractive\",\n",
        "          \"wavy-hair\",\n",
        "          \"young\",\n",
        "          \"big lips\",\n",
        "          \"big nose\",\n",
        "          \"chubby\",\n",
        "          \"glasses\",\n",
        "          ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKalZXrRt0gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadNext(index=0):\n",
        "  #Loop into the images of the path specs\n",
        "    img = np.asarray(Image.open(path + '/' + paths[index]))\n",
        "    current_file.value = paths[index]\n",
        "    if len(paths) == 0:\n",
        "        paths.extend(paths_backup)\n",
        "    if img.shape[2] == 4:\n",
        "        img = img[:, :, :3]\n",
        "    im = img.transpose((2, 0, 1))\n",
        "    x = torch.tensor(np.asarray(im, dtype=np.float32), device='cpu', requires_grad=True).cuda() / 127.5 - 1.\n",
        "    if x.shape[0] == 4:\n",
        "        x = x[:3]\n",
        "    needed_resolution = model.decoder.layer_to_resolution[-1]\n",
        "    while x.shape[2] > needed_resolution:\n",
        "        x = F.avg_pool2d(x, 2, 2)\n",
        "    if x.shape[2] != needed_resolution:\n",
        "        x = F.adaptive_avg_pool2d(x, (needed_resolution, needed_resolution))\n",
        "    img_src = ((x * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255).cpu().type(torch.uint8).transpose(0, 2).transpose(0, 1).numpy()\n",
        "    latents_original = encode(x[None, ...].cuda())\n",
        "    latents = latents_original[0, 0].clone()\n",
        "    latents -= model.dlatent_avg.buff.data[0]   \n",
        "    for v, w in zip(attribute_values, W):\n",
        "        v.value = (latents * w).sum()\n",
        "    for v, w in zip(attribute_values, W):\n",
        "        latents = latents - v.value * w\n",
        "    return latents, latents_original, img_src\n",
        "\n",
        "def loadRandom():\n",
        "        latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n",
        "        lat = torch.tensor(latents).float().cuda()\n",
        "        dlat = mapping_fl(lat)\n",
        "        layer_idx = torch.arange(2 * layer_count)[np.newaxis, :, np.newaxis]\n",
        "        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n",
        "        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n",
        "        dlat = torch.lerp(model.dlatent_avg.buff.data, dlat, coefs)\n",
        "        x = decode(dlat)[0]\n",
        "        img_src = ((x * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255).cpu().type(torch.uint8).transpose(0, 2).transpose(0, 1).numpy()\n",
        "        latents_original = dlat\n",
        "        latents = latents_original[0, 0].clone()\n",
        "        latents -= model.dlatent_avg.buff.data[0]\n",
        "        \n",
        "        for v, w in zip(attribute_values, W):\n",
        "            v.value = (latents * w).sum()\n",
        "\n",
        "        for v, w in zip(attribute_values, W):\n",
        "            latents = latents - v.value * w\n",
        "\n",
        "        return latents, latents_original, img_src\n",
        "    \n",
        "def update_image(w, latents_original):\n",
        "    with torch.no_grad():\n",
        "        w = w + model.dlatent_avg.buff.data[0]\n",
        "        w = w[None, None, ...].repeat(1, model.mapping_fl.num_layers, 1)\n",
        "\n",
        "        layer_idx = torch.arange(model.mapping_fl.num_layers)[np.newaxis, :, np.newaxis]\n",
        "        cur_layers = (7 + 1) * 2\n",
        "        mixing_cutoff = cur_layers\n",
        "        styles = torch.where(layer_idx < mixing_cutoff, w, latents_original)\n",
        "\n",
        "        x_rec = decode(styles)\n",
        "        resultsample = ((x_rec * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255)\n",
        "        resultsample = resultsample.cpu()[0, :, :, :]\n",
        "        return resultsample.type(torch.uint8).transpose(0, 2).transpose(0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtkqzaoKt0gX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.set_device(0)\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "cfg = get_cfg_defaults()\n",
        "cfg.merge_from_file(\"./configs/ffhq.yaml\")\n",
        "logger = logging.getLogger(\"logger\")\n",
        "logger.setLevel(logging.DEBUG)\n",
        "model = Model(\n",
        "    startf=cfg.MODEL.START_CHANNEL_COUNT,\n",
        "    layer_count=cfg.MODEL.LAYER_COUNT,\n",
        "    maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n",
        "    latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n",
        "    truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n",
        "    truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n",
        "    mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n",
        "    channels=cfg.MODEL.CHANNELS,\n",
        "    generator=cfg.MODEL.GENERATOR,\n",
        "    encoder=cfg.MODEL.ENCODER)\n",
        "\n",
        "model.cuda()\n",
        "model.eval()\n",
        "model.requires_grad_(False)\n",
        "\n",
        "decoder = model.decoder\n",
        "encoder = model.encoder\n",
        "mapping_tl = model.mapping_tl\n",
        "mapping_fl = model.mapping_fl\n",
        "dlatent_avg = model.dlatent_avg\n",
        "\n",
        "logger.info(\"Trainable parameters generator:\")\n",
        "count_parameters(decoder)\n",
        "\n",
        "logger.info(\"Trainable parameters discriminator:\")\n",
        "count_parameters(encoder)\n",
        "\n",
        "arguments = dict()\n",
        "arguments[\"iteration\"] = 0\n",
        "\n",
        "model_dict = {\n",
        "    'discriminator_s': encoder,\n",
        "    'generator_s': decoder,\n",
        "    'mapping_tl_s': mapping_tl,\n",
        "    'mapping_fl_s': mapping_fl,\n",
        "    'dlatent_avg': dlatent_avg\n",
        "}\n",
        "\n",
        "checkpointer = Checkpointer(cfg,\n",
        "                            model_dict,\n",
        "                            {},\n",
        "                            logger=logger,\n",
        "                            save=False)\n",
        "\n",
        "extra_checkpoint_data = checkpointer.load()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "layer_count = cfg.MODEL.LAYER_COUNT\n",
        "\n",
        "\n",
        "def encode(x):\n",
        "    Z, _ = model.encode(x, layer_count - 1, 1)\n",
        "    Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n",
        "    # print(Z.shape)\n",
        "    return Z\n",
        "\n",
        "\n",
        "def decode(x):\n",
        "    layer_idx = torch.arange(2 * layer_count)[np.newaxis, :, np.newaxis]\n",
        "    ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n",
        "    coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n",
        "    # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n",
        "    return model.decoder(x, layer_count - 1, 1, noise=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmBYpsi6t0gj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1542ca9d-fbae-4505-fd53-a7b346813405"
      },
      "source": [
        "!mkdir 'dataset_samples/faces/test_images'\n",
        "path = 'dataset_samples/faces/test_images'\n",
        "\n",
        "paths = list(os.listdir(path))\n",
        "paths.sort()\n",
        "paths_backup = paths[:]\n",
        "\n",
        "\n",
        "randomize = bimpy.Bool(True)\n",
        "current_file = bimpy.String(\"\")\n",
        "\n",
        "ctx = bimpy.Context()\n",
        "\n",
        "attribute_values = [bimpy.Float(0) for i in indices]\n",
        "\n",
        "# W: 9x512\n",
        "W = [torch.tensor(np.load(\"principal_directions/direction_%d.npy\" % i), dtype=torch.float32) for i in indices]\n",
        "\n",
        "rnd = np.random.RandomState(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘dataset_samples/faces/test_images’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "JNjfL_ZNt0gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im_size = 2 ** (cfg.MODEL.LAYER_COUNT + 1)\n",
        "seed = 0\n",
        "\n",
        "#image_index = 6 # image index \n",
        "slider_vals = np.linspace(-20, 20, 10) # simulate the slider form interactive demo\n",
        "\n",
        "for image_index in range(2):\n",
        "    for target_attr in range(len(labels)):\n",
        "        latents, latents_original, img_src = loadNext(image_index)  \n",
        "\n",
        "        fig, ax = plt.subplots(1, len(slider_vals)+1, figsize=(25, 6))\n",
        "        fig.suptitle(f\"Variation across: {labels[target_attr]}\", y=0.7)\n",
        "        ax[0].imshow(img_src)\n",
        "        ax[0].set_title(\"Original image\")\n",
        "        ax[0].axis('off')\n",
        "\n",
        "        for i, val in enumerate(slider_vals):\n",
        "            attribute_values[target_attr].value = val\n",
        "            new_latents = latents + sum([v.value * w for v, w in zip(attribute_values, W)])\n",
        "            new_im = update_image(new_latents, latents_original)\n",
        "\n",
        "            ax[i+1].imshow(new_im)\n",
        "            ax[i+1].set_title(round(val, 1))\n",
        "            ax[i+1].axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}